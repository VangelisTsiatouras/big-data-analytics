{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: swifter in /home/vangelis/.local/share/virtualenvs/big-data-analytics/lib/python3.8/site-packages (1.0.7)\n",
      "Requirement already satisfied: numpy in /home/vangelis/.local/share/virtualenvs/big-data-analytics/lib/python3.8/site-packages (1.19.5)\n",
      "Requirement already satisfied: pandas in /home/vangelis/.local/share/virtualenvs/big-data-analytics/lib/python3.8/site-packages (1.1.5)\n",
      "Requirement already satisfied: datasketch in /home/vangelis/.local/share/virtualenvs/big-data-analytics/lib/python3.8/site-packages (1.5.3)\n",
      "Requirement already satisfied: scikit-learn in /home/vangelis/.local/share/virtualenvs/big-data-analytics/lib/python3.8/site-packages (0.24.1)\n",
      "Requirement already satisfied: matplotlib in /home/vangelis/.local/share/virtualenvs/big-data-analytics/lib/python3.8/site-packages (3.3.3)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /home/vangelis/.local/share/virtualenvs/big-data-analytics/lib/python3.8/site-packages (from matplotlib) (1.3.1)\n",
      "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.3 in /home/vangelis/.local/share/virtualenvs/big-data-analytics/lib/python3.8/site-packages (from matplotlib) (2.4.7)\n",
      "Requirement already satisfied: pillow>=6.2.0 in /home/vangelis/.local/share/virtualenvs/big-data-analytics/lib/python3.8/site-packages (from matplotlib) (8.1.0)\n",
      "Requirement already satisfied: python-dateutil>=2.1 in /home/vangelis/.local/share/virtualenvs/big-data-analytics/lib/python3.8/site-packages (from matplotlib) (2.8.1)\n",
      "Requirement already satisfied: cycler>=0.10 in /home/vangelis/.local/share/virtualenvs/big-data-analytics/lib/python3.8/site-packages (from matplotlib) (0.10.0)\n",
      "Requirement already satisfied: six in /home/vangelis/.local/share/virtualenvs/big-data-analytics/lib/python3.8/site-packages (from cycler>=0.10->matplotlib) (1.15.0)\n",
      "Requirement already satisfied: pytz>=2017.2 in /home/vangelis/.local/share/virtualenvs/big-data-analytics/lib/python3.8/site-packages (from pandas) (2020.5)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /home/vangelis/.local/share/virtualenvs/big-data-analytics/lib/python3.8/site-packages (from scikit-learn) (2.1.0)\n",
      "Requirement already satisfied: scipy>=0.19.1 in /home/vangelis/.local/share/virtualenvs/big-data-analytics/lib/python3.8/site-packages (from scikit-learn) (1.6.0)\n",
      "Requirement already satisfied: joblib>=0.11 in /home/vangelis/.local/share/virtualenvs/big-data-analytics/lib/python3.8/site-packages (from scikit-learn) (1.0.0)\n",
      "Requirement already satisfied: psutil>=5.6.6 in /home/vangelis/.local/share/virtualenvs/big-data-analytics/lib/python3.8/site-packages (from swifter) (5.8.0)\n",
      "Requirement already satisfied: dask[dataframe]>=2.10.0 in /home/vangelis/.local/share/virtualenvs/big-data-analytics/lib/python3.8/site-packages (from swifter) (2021.1.1)\n",
      "Requirement already satisfied: tqdm>=4.33.0 in /home/vangelis/.local/share/virtualenvs/big-data-analytics/lib/python3.8/site-packages (from swifter) (4.56.0)\n",
      "Requirement already satisfied: ipywidgets>=7.0.0cloudpickle>=0.2.2 in /home/vangelis/.local/share/virtualenvs/big-data-analytics/lib/python3.8/site-packages (from swifter) (7.6.3)\n",
      "Requirement already satisfied: parso>0.4.0 in /home/vangelis/.local/share/virtualenvs/big-data-analytics/lib/python3.8/site-packages (from swifter) (0.8.1)\n",
      "Requirement already satisfied: bleach>=3.1.1 in /home/vangelis/.local/share/virtualenvs/big-data-analytics/lib/python3.8/site-packages (from swifter) (3.2.3)\n",
      "Requirement already satisfied: modin[ray]>=0.8.1.1 in /home/vangelis/.local/share/virtualenvs/big-data-analytics/lib/python3.8/site-packages (from swifter) (0.8.3)\n",
      "Requirement already satisfied: webencodings in /home/vangelis/.local/share/virtualenvs/big-data-analytics/lib/python3.8/site-packages (from bleach>=3.1.1->swifter) (0.5.1)\n",
      "Requirement already satisfied: packaging in /home/vangelis/.local/share/virtualenvs/big-data-analytics/lib/python3.8/site-packages (from bleach>=3.1.1->swifter) (20.8)\n",
      "Requirement already satisfied: pyyaml in /home/vangelis/.local/share/virtualenvs/big-data-analytics/lib/python3.8/site-packages (from dask[dataframe]>=2.10.0->swifter) (5.4.1)\n",
      "Requirement already satisfied: fsspec>=0.6.0 in /home/vangelis/.local/share/virtualenvs/big-data-analytics/lib/python3.8/site-packages (from dask[dataframe]>=2.10.0->swifter) (0.8.5)\n",
      "Requirement already satisfied: toolz>=0.8.2 in /home/vangelis/.local/share/virtualenvs/big-data-analytics/lib/python3.8/site-packages (from dask[dataframe]>=2.10.0->swifter) (0.11.1)\n",
      "Requirement already satisfied: partd>=0.3.10 in /home/vangelis/.local/share/virtualenvs/big-data-analytics/lib/python3.8/site-packages (from dask[dataframe]>=2.10.0->swifter) (1.1.0)\n",
      "Requirement already satisfied: widgetsnbextension~=3.5.0 in /home/vangelis/.local/share/virtualenvs/big-data-analytics/lib/python3.8/site-packages (from ipywidgets>=7.0.0cloudpickle>=0.2.2->swifter) (3.5.1)\n",
      "Requirement already satisfied: ipykernel>=4.5.1 in /home/vangelis/.local/share/virtualenvs/big-data-analytics/lib/python3.8/site-packages (from ipywidgets>=7.0.0cloudpickle>=0.2.2->swifter) (5.4.3)\n",
      "Requirement already satisfied: jupyterlab-widgets>=1.0.0 in /home/vangelis/.local/share/virtualenvs/big-data-analytics/lib/python3.8/site-packages (from ipywidgets>=7.0.0cloudpickle>=0.2.2->swifter) (1.0.0)\n",
      "Requirement already satisfied: nbformat>=4.2.0 in /home/vangelis/.local/share/virtualenvs/big-data-analytics/lib/python3.8/site-packages (from ipywidgets>=7.0.0cloudpickle>=0.2.2->swifter) (5.1.2)\n",
      "Requirement already satisfied: ipython>=4.0.0 in /home/vangelis/.local/share/virtualenvs/big-data-analytics/lib/python3.8/site-packages (from ipywidgets>=7.0.0cloudpickle>=0.2.2->swifter) (7.19.0)\n",
      "Requirement already satisfied: traitlets>=4.3.1 in /home/vangelis/.local/share/virtualenvs/big-data-analytics/lib/python3.8/site-packages (from ipywidgets>=7.0.0cloudpickle>=0.2.2->swifter) (5.0.5)\n",
      "Requirement already satisfied: jupyter-client in /home/vangelis/.local/share/virtualenvs/big-data-analytics/lib/python3.8/site-packages (from ipykernel>=4.5.1->ipywidgets>=7.0.0cloudpickle>=0.2.2->swifter) (6.1.11)\n",
      "Requirement already satisfied: tornado>=4.2 in /home/vangelis/.local/share/virtualenvs/big-data-analytics/lib/python3.8/site-packages (from ipykernel>=4.5.1->ipywidgets>=7.0.0cloudpickle>=0.2.2->swifter) (6.1)\n",
      "Requirement already satisfied: decorator in /home/vangelis/.local/share/virtualenvs/big-data-analytics/lib/python3.8/site-packages (from ipython>=4.0.0->ipywidgets>=7.0.0cloudpickle>=0.2.2->swifter) (4.4.2)\n",
      "Requirement already satisfied: backcall in /home/vangelis/.local/share/virtualenvs/big-data-analytics/lib/python3.8/site-packages (from ipython>=4.0.0->ipywidgets>=7.0.0cloudpickle>=0.2.2->swifter) (0.2.0)\n",
      "Requirement already satisfied: pickleshare in /home/vangelis/.local/share/virtualenvs/big-data-analytics/lib/python3.8/site-packages (from ipython>=4.0.0->ipywidgets>=7.0.0cloudpickle>=0.2.2->swifter) (0.7.5)\n",
      "Requirement already satisfied: pexpect>4.3 in /home/vangelis/.local/share/virtualenvs/big-data-analytics/lib/python3.8/site-packages (from ipython>=4.0.0->ipywidgets>=7.0.0cloudpickle>=0.2.2->swifter) (4.8.0)\n",
      "Requirement already satisfied: prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0 in /home/vangelis/.local/share/virtualenvs/big-data-analytics/lib/python3.8/site-packages (from ipython>=4.0.0->ipywidgets>=7.0.0cloudpickle>=0.2.2->swifter) (3.0.14)\n",
      "Requirement already satisfied: pygments in /home/vangelis/.local/share/virtualenvs/big-data-analytics/lib/python3.8/site-packages (from ipython>=4.0.0->ipywidgets>=7.0.0cloudpickle>=0.2.2->swifter) (2.7.4)\n",
      "Requirement already satisfied: setuptools>=18.5 in /home/vangelis/.local/share/virtualenvs/big-data-analytics/lib/python3.8/site-packages (from ipython>=4.0.0->ipywidgets>=7.0.0cloudpickle>=0.2.2->swifter) (51.1.1)\n",
      "Requirement already satisfied: jedi>=0.10 in /home/vangelis/.local/share/virtualenvs/big-data-analytics/lib/python3.8/site-packages (from ipython>=4.0.0->ipywidgets>=7.0.0cloudpickle>=0.2.2->swifter) (0.18.0)\n",
      "Requirement already satisfied: pyarrow==1.0 in /home/vangelis/.local/share/virtualenvs/big-data-analytics/lib/python3.8/site-packages (from modin[ray]>=0.8.1.1->swifter) (1.0.0)\n",
      "Requirement already satisfied: ray>=1.0.0 in /home/vangelis/.local/share/virtualenvs/big-data-analytics/lib/python3.8/site-packages (from modin[ray]>=0.8.1.1->swifter) (1.1.0)\n",
      "Requirement already satisfied: jsonschema!=2.5.0,>=2.4 in /home/vangelis/.local/share/virtualenvs/big-data-analytics/lib/python3.8/site-packages (from nbformat>=4.2.0->ipywidgets>=7.0.0cloudpickle>=0.2.2->swifter) (3.2.0)\n",
      "Requirement already satisfied: jupyter-core in /home/vangelis/.local/share/virtualenvs/big-data-analytics/lib/python3.8/site-packages (from nbformat>=4.2.0->ipywidgets>=7.0.0cloudpickle>=0.2.2->swifter) (4.7.0)\n",
      "Requirement already satisfied: ipython-genutils in /home/vangelis/.local/share/virtualenvs/big-data-analytics/lib/python3.8/site-packages (from nbformat>=4.2.0->ipywidgets>=7.0.0cloudpickle>=0.2.2->swifter) (0.2.0)\n",
      "Requirement already satisfied: pyrsistent>=0.14.0 in /home/vangelis/.local/share/virtualenvs/big-data-analytics/lib/python3.8/site-packages (from jsonschema!=2.5.0,>=2.4->nbformat>=4.2.0->ipywidgets>=7.0.0cloudpickle>=0.2.2->swifter) (0.17.3)\n",
      "Requirement already satisfied: attrs>=17.4.0 in /home/vangelis/.local/share/virtualenvs/big-data-analytics/lib/python3.8/site-packages (from jsonschema!=2.5.0,>=2.4->nbformat>=4.2.0->ipywidgets>=7.0.0cloudpickle>=0.2.2->swifter) (20.3.0)\n",
      "Requirement already satisfied: locket in /home/vangelis/.local/share/virtualenvs/big-data-analytics/lib/python3.8/site-packages (from partd>=0.3.10->dask[dataframe]>=2.10.0->swifter) (0.2.1)\n",
      "Requirement already satisfied: ptyprocess>=0.5 in /home/vangelis/.local/share/virtualenvs/big-data-analytics/lib/python3.8/site-packages (from pexpect>4.3->ipython>=4.0.0->ipywidgets>=7.0.0cloudpickle>=0.2.2->swifter) (0.7.0)\n",
      "Requirement already satisfied: wcwidth in /home/vangelis/.local/share/virtualenvs/big-data-analytics/lib/python3.8/site-packages (from prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0->ipython>=4.0.0->ipywidgets>=7.0.0cloudpickle>=0.2.2->swifter) (0.2.5)\n",
      "Requirement already satisfied: colorful in /home/vangelis/.local/share/virtualenvs/big-data-analytics/lib/python3.8/site-packages (from ray>=1.0.0->modin[ray]>=0.8.1.1->swifter) (0.5.4)\n",
      "Requirement already satisfied: click>=7.0 in /home/vangelis/.local/share/virtualenvs/big-data-analytics/lib/python3.8/site-packages (from ray>=1.0.0->modin[ray]>=0.8.1.1->swifter) (7.1.2)\n",
      "Requirement already satisfied: gpustat in /home/vangelis/.local/share/virtualenvs/big-data-analytics/lib/python3.8/site-packages (from ray>=1.0.0->modin[ray]>=0.8.1.1->swifter) (0.6.0)\n",
      "Requirement already satisfied: opencensus in /home/vangelis/.local/share/virtualenvs/big-data-analytics/lib/python3.8/site-packages (from ray>=1.0.0->modin[ray]>=0.8.1.1->swifter) (0.7.12)\n",
      "Requirement already satisfied: protobuf>=3.8.0 in /home/vangelis/.local/share/virtualenvs/big-data-analytics/lib/python3.8/site-packages (from ray>=1.0.0->modin[ray]>=0.8.1.1->swifter) (3.14.0)\n",
      "Requirement already satisfied: filelock in /home/vangelis/.local/share/virtualenvs/big-data-analytics/lib/python3.8/site-packages (from ray>=1.0.0->modin[ray]>=0.8.1.1->swifter) (3.0.12)\n",
      "Requirement already satisfied: aioredis in /home/vangelis/.local/share/virtualenvs/big-data-analytics/lib/python3.8/site-packages (from ray>=1.0.0->modin[ray]>=0.8.1.1->swifter) (1.3.1)\n",
      "Requirement already satisfied: grpcio>=1.28.1 in /home/vangelis/.local/share/virtualenvs/big-data-analytics/lib/python3.8/site-packages (from ray>=1.0.0->modin[ray]>=0.8.1.1->swifter) (1.35.0)\n",
      "Requirement already satisfied: msgpack<2.0.0,>=1.0.0 in /home/vangelis/.local/share/virtualenvs/big-data-analytics/lib/python3.8/site-packages (from ray>=1.0.0->modin[ray]>=0.8.1.1->swifter) (1.0.2)\n",
      "Requirement already satisfied: requests in /home/vangelis/.local/share/virtualenvs/big-data-analytics/lib/python3.8/site-packages (from ray>=1.0.0->modin[ray]>=0.8.1.1->swifter) (2.25.1)\n",
      "Requirement already satisfied: aiohttp-cors in /home/vangelis/.local/share/virtualenvs/big-data-analytics/lib/python3.8/site-packages (from ray>=1.0.0->modin[ray]>=0.8.1.1->swifter) (0.7.0)\n",
      "Requirement already satisfied: redis>=3.5.0 in /home/vangelis/.local/share/virtualenvs/big-data-analytics/lib/python3.8/site-packages (from ray>=1.0.0->modin[ray]>=0.8.1.1->swifter) (3.5.3)\n",
      "Requirement already satisfied: prometheus-client>=0.7.1 in /home/vangelis/.local/share/virtualenvs/big-data-analytics/lib/python3.8/site-packages (from ray>=1.0.0->modin[ray]>=0.8.1.1->swifter) (0.9.0)\n",
      "Requirement already satisfied: colorama in /home/vangelis/.local/share/virtualenvs/big-data-analytics/lib/python3.8/site-packages (from ray>=1.0.0->modin[ray]>=0.8.1.1->swifter) (0.4.4)\n",
      "Requirement already satisfied: aiohttp in /home/vangelis/.local/share/virtualenvs/big-data-analytics/lib/python3.8/site-packages (from ray>=1.0.0->modin[ray]>=0.8.1.1->swifter) (3.7.3)\n",
      "Requirement already satisfied: py-spy>=0.2.0 in /home/vangelis/.local/share/virtualenvs/big-data-analytics/lib/python3.8/site-packages (from ray>=1.0.0->modin[ray]>=0.8.1.1->swifter) (0.3.4)\n",
      "Requirement already satisfied: notebook>=4.4.1 in /home/vangelis/.local/share/virtualenvs/big-data-analytics/lib/python3.8/site-packages (from widgetsnbextension~=3.5.0->ipywidgets>=7.0.0cloudpickle>=0.2.2->swifter) (6.2.0)\n",
      "Requirement already satisfied: nbconvert in /home/vangelis/.local/share/virtualenvs/big-data-analytics/lib/python3.8/site-packages (from notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets>=7.0.0cloudpickle>=0.2.2->swifter) (6.0.7)\n",
      "Requirement already satisfied: terminado>=0.8.3 in /home/vangelis/.local/share/virtualenvs/big-data-analytics/lib/python3.8/site-packages (from notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets>=7.0.0cloudpickle>=0.2.2->swifter) (0.9.2)\n",
      "Requirement already satisfied: Send2Trash>=1.5.0 in /home/vangelis/.local/share/virtualenvs/big-data-analytics/lib/python3.8/site-packages (from notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets>=7.0.0cloudpickle>=0.2.2->swifter) (1.5.0)\n",
      "Requirement already satisfied: jinja2 in /home/vangelis/.local/share/virtualenvs/big-data-analytics/lib/python3.8/site-packages (from notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets>=7.0.0cloudpickle>=0.2.2->swifter) (2.11.2)\n",
      "Requirement already satisfied: pyzmq>=17 in /home/vangelis/.local/share/virtualenvs/big-data-analytics/lib/python3.8/site-packages (from notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets>=7.0.0cloudpickle>=0.2.2->swifter) (21.0.2)\n",
      "Requirement already satisfied: argon2-cffi in /home/vangelis/.local/share/virtualenvs/big-data-analytics/lib/python3.8/site-packages (from notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets>=7.0.0cloudpickle>=0.2.2->swifter) (20.1.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /home/vangelis/.local/share/virtualenvs/big-data-analytics/lib/python3.8/site-packages (from aiohttp->ray>=1.0.0->modin[ray]>=0.8.1.1->swifter) (5.1.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.5 in /home/vangelis/.local/share/virtualenvs/big-data-analytics/lib/python3.8/site-packages (from aiohttp->ray>=1.0.0->modin[ray]>=0.8.1.1->swifter) (3.7.4.3)\n",
      "Requirement already satisfied: async-timeout<4.0,>=3.0 in /home/vangelis/.local/share/virtualenvs/big-data-analytics/lib/python3.8/site-packages (from aiohttp->ray>=1.0.0->modin[ray]>=0.8.1.1->swifter) (3.0.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /home/vangelis/.local/share/virtualenvs/big-data-analytics/lib/python3.8/site-packages (from aiohttp->ray>=1.0.0->modin[ray]>=0.8.1.1->swifter) (1.6.3)\n",
      "Requirement already satisfied: chardet<4.0,>=2.0 in /home/vangelis/.local/share/virtualenvs/big-data-analytics/lib/python3.8/site-packages (from aiohttp->ray>=1.0.0->modin[ray]>=0.8.1.1->swifter) (3.0.4)\n",
      "Requirement already satisfied: idna>=2.0 in /home/vangelis/.local/share/virtualenvs/big-data-analytics/lib/python3.8/site-packages (from yarl<2.0,>=1.0->aiohttp->ray>=1.0.0->modin[ray]>=0.8.1.1->swifter) (2.10)\n",
      "Requirement already satisfied: hiredis in /home/vangelis/.local/share/virtualenvs/big-data-analytics/lib/python3.8/site-packages (from aioredis->ray>=1.0.0->modin[ray]>=0.8.1.1->swifter) (1.1.0)\n",
      "Requirement already satisfied: cffi>=1.0.0 in /home/vangelis/.local/share/virtualenvs/big-data-analytics/lib/python3.8/site-packages (from argon2-cffi->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets>=7.0.0cloudpickle>=0.2.2->swifter) (1.14.4)\n",
      "Requirement already satisfied: pycparser in /home/vangelis/.local/share/virtualenvs/big-data-analytics/lib/python3.8/site-packages (from cffi>=1.0.0->argon2-cffi->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets>=7.0.0cloudpickle>=0.2.2->swifter) (2.20)\n",
      "Requirement already satisfied: nvidia-ml-py3>=7.352.0 in /home/vangelis/.local/share/virtualenvs/big-data-analytics/lib/python3.8/site-packages (from gpustat->ray>=1.0.0->modin[ray]>=0.8.1.1->swifter) (7.352.0)\n",
      "Requirement already satisfied: blessings>=1.6 in /home/vangelis/.local/share/virtualenvs/big-data-analytics/lib/python3.8/site-packages (from gpustat->ray>=1.0.0->modin[ray]>=0.8.1.1->swifter) (1.7)\n",
      "Requirement already satisfied: MarkupSafe>=0.23 in /home/vangelis/.local/share/virtualenvs/big-data-analytics/lib/python3.8/site-packages (from jinja2->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets>=7.0.0cloudpickle>=0.2.2->swifter) (1.1.1)\n",
      "Requirement already satisfied: mistune<2,>=0.8.1 in /home/vangelis/.local/share/virtualenvs/big-data-analytics/lib/python3.8/site-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets>=7.0.0cloudpickle>=0.2.2->swifter) (0.8.4)\n",
      "Requirement already satisfied: jupyterlab-pygments in /home/vangelis/.local/share/virtualenvs/big-data-analytics/lib/python3.8/site-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets>=7.0.0cloudpickle>=0.2.2->swifter) (0.1.2)\n",
      "Requirement already satisfied: nbclient<0.6.0,>=0.5.0 in /home/vangelis/.local/share/virtualenvs/big-data-analytics/lib/python3.8/site-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets>=7.0.0cloudpickle>=0.2.2->swifter) (0.5.1)\n",
      "Requirement already satisfied: defusedxml in /home/vangelis/.local/share/virtualenvs/big-data-analytics/lib/python3.8/site-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets>=7.0.0cloudpickle>=0.2.2->swifter) (0.6.0)\n",
      "Requirement already satisfied: testpath in /home/vangelis/.local/share/virtualenvs/big-data-analytics/lib/python3.8/site-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets>=7.0.0cloudpickle>=0.2.2->swifter) (0.4.4)\n",
      "Requirement already satisfied: pandocfilters>=1.4.1 in /home/vangelis/.local/share/virtualenvs/big-data-analytics/lib/python3.8/site-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets>=7.0.0cloudpickle>=0.2.2->swifter) (1.4.3)\n",
      "Requirement already satisfied: entrypoints>=0.2.2 in /home/vangelis/.local/share/virtualenvs/big-data-analytics/lib/python3.8/site-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets>=7.0.0cloudpickle>=0.2.2->swifter) (0.3)\n",
      "Requirement already satisfied: nest-asyncio in /home/vangelis/.local/share/virtualenvs/big-data-analytics/lib/python3.8/site-packages (from nbclient<0.6.0,>=0.5.0->nbconvert->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets>=7.0.0cloudpickle>=0.2.2->swifter) (1.4.3)\n",
      "Requirement already satisfied: async-generator in /home/vangelis/.local/share/virtualenvs/big-data-analytics/lib/python3.8/site-packages (from nbclient<0.6.0,>=0.5.0->nbconvert->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets>=7.0.0cloudpickle>=0.2.2->swifter) (1.10)\n",
      "Requirement already satisfied: opencensus-context==0.1.2 in /home/vangelis/.local/share/virtualenvs/big-data-analytics/lib/python3.8/site-packages (from opencensus->ray>=1.0.0->modin[ray]>=0.8.1.1->swifter) (0.1.2)\n",
      "Requirement already satisfied: google-api-core<2.0.0,>=1.0.0 in /home/vangelis/.local/share/virtualenvs/big-data-analytics/lib/python3.8/site-packages (from opencensus->ray>=1.0.0->modin[ray]>=0.8.1.1->swifter) (1.25.1)\n",
      "Requirement already satisfied: google-auth<2.0dev,>=1.21.1 in /home/vangelis/.local/share/virtualenvs/big-data-analytics/lib/python3.8/site-packages (from google-api-core<2.0.0,>=1.0.0->opencensus->ray>=1.0.0->modin[ray]>=0.8.1.1->swifter) (1.24.0)\n",
      "Requirement already satisfied: googleapis-common-protos<2.0dev,>=1.6.0 in /home/vangelis/.local/share/virtualenvs/big-data-analytics/lib/python3.8/site-packages (from google-api-core<2.0.0,>=1.0.0->opencensus->ray>=1.0.0->modin[ray]>=0.8.1.1->swifter) (1.52.0)\n",
      "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /home/vangelis/.local/share/virtualenvs/big-data-analytics/lib/python3.8/site-packages (from google-auth<2.0dev,>=1.21.1->google-api-core<2.0.0,>=1.0.0->opencensus->ray>=1.0.0->modin[ray]>=0.8.1.1->swifter) (4.2.1)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /home/vangelis/.local/share/virtualenvs/big-data-analytics/lib/python3.8/site-packages (from google-auth<2.0dev,>=1.21.1->google-api-core<2.0.0,>=1.0.0->opencensus->ray>=1.0.0->modin[ray]>=0.8.1.1->swifter) (0.2.8)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /home/vangelis/.local/share/virtualenvs/big-data-analytics/lib/python3.8/site-packages (from google-auth<2.0dev,>=1.21.1->google-api-core<2.0.0,>=1.0.0->opencensus->ray>=1.0.0->modin[ray]>=0.8.1.1->swifter) (4.7)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /home/vangelis/.local/share/virtualenvs/big-data-analytics/lib/python3.8/site-packages (from pyasn1-modules>=0.2.1->google-auth<2.0dev,>=1.21.1->google-api-core<2.0.0,>=1.0.0->opencensus->ray>=1.0.0->modin[ray]>=0.8.1.1->swifter) (0.4.8)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/vangelis/.local/share/virtualenvs/big-data-analytics/lib/python3.8/site-packages (from requests->ray>=1.0.0->modin[ray]>=0.8.1.1->swifter) (2020.12.5)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /home/vangelis/.local/share/virtualenvs/big-data-analytics/lib/python3.8/site-packages (from requests->ray>=1.0.0->modin[ray]>=0.8.1.1->swifter) (1.26.3)\n",
      "\u001b[33mWARNING: You are using pip version 21.0; however, version 21.0.1 is available.\n",
      "You should consider upgrading via the '/home/vangelis/.virtualenvs/big-data-analytics/bin/python -m pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "! pip install swifter numpy pandas datasketch scikit-learn matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "475e7cc1d2784fe2909f06be66f6218d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Pandas Apply:   0%|          | 0/531990 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bad353ef32ec4ed581fcc2fee6762fe6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Pandas Apply:   0%|          | 0/5374 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query: Pandas(id=0, content='how many people are going towards using phones to search for local businesses')\n",
      "Series([], Name: content, dtype: object)\n",
      "Query: Pandas(id=1, content='can an android app use sms only to communicate questions to an intelligence engine without wifi and get back info')\n",
      "Series([], Name: content, dtype: object)\n",
      "Query: Pandas(id=2, content='what small detail from an indian movie do you love')\n",
      "Series([], Name: content, dtype: object)\n",
      "Query: Pandas(id=3, content='why can not hindu women be the soldier of hinduism why cant she give birth to a hindu kid even if she marries a non-hindu')\n",
      "Series([], Name: content, dtype: object)\n",
      "Query: Pandas(id=4, content='how would you write out twelve lakh twelve thousand twelve hundred and twelve numerically')\n",
      "Series([], Name: content, dtype: object)\n",
      "Query: Pandas(id=5, content='what are the rto formalities to transfer a car from pune mh 12 passing to bangalore')\n",
      "Series([], Name: content, dtype: object)\n",
      "Query: Pandas(id=6, content='do real transsexuals and shemale porn have nothing in common')\n",
      "Series([], Name: content, dtype: object)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m~/.virtualenvs/big-data-analytics/lib/python3.8/site-packages/swifter/swifter.py\u001b[0m in \u001b[0;36mapply\u001b[0;34m(self, func, convert_dtype, args, **kwds)\u001b[0m\n\u001b[1;32m    218\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0msuppress_stdout_stderr_logging\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 219\u001b[0;31m                 \u001b[0mtmp_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    220\u001b[0m                 \u001b[0msample_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msample\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconvert_dtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconvert_dtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-2-4a4188a25a16>\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(row)\u001b[0m\n\u001b[1;32m     89\u001b[0m     similarities = train_df['content'].swifter.progress_bar(False).apply(\n\u001b[0;32m---> 90\u001b[0;31m         lambda row: jaccard_similarity(row, query.content))\n\u001b[0m\u001b[1;32m     91\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0many\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msimilarities\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0mTHRESHOLD\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-2-4a4188a25a16>\u001b[0m in \u001b[0;36mjaccard_similarity\u001b[0;34m(query, document)\u001b[0m\n\u001b[1;32m     77\u001b[0m     \"\"\"\n\u001b[0;32m---> 78\u001b[0;31m     \u001b[0mquery_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mquery\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     79\u001b[0m     \u001b[0mdocument_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdocument\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.virtualenvs/big-data-analytics/lib/python3.8/site-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   5140\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 5141\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mobject\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getattribute__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   5142\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'Series' object has no attribute 'split'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-4a4188a25a16>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     87\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     88\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mquery\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtest_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitertuples\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 89\u001b[0;31m     similarities = train_df['content'].swifter.progress_bar(False).apply(\n\u001b[0m\u001b[1;32m     90\u001b[0m         lambda row: jaccard_similarity(row, query.content))\n\u001b[1;32m     91\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0many\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msimilarities\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0mTHRESHOLD\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.virtualenvs/big-data-analytics/lib/python3.8/site-packages/swifter/swifter.py\u001b[0m in \u001b[0;36mapply\u001b[0;34m(self, func, convert_dtype, args, **kwds)\u001b[0m\n\u001b[1;32m    238\u001b[0m                     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_obj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprogress_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconvert_dtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconvert_dtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    239\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 240\u001b[0;31m                     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_obj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconvert_dtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconvert_dtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    241\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    242\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.virtualenvs/big-data-analytics/lib/python3.8/site-packages/pandas/core/series.py\u001b[0m in \u001b[0;36mapply\u001b[0;34m(self, func, convert_dtype, args, **kwds)\u001b[0m\n\u001b[1;32m   4211\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4212\u001b[0m                 \u001b[0mvalues\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobject\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_values\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4213\u001b[0;31m                 \u001b[0mmapped\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap_infer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconvert\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconvert_dtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4214\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4215\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmapped\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmapped\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mSeries\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/lib.pyx\u001b[0m in \u001b[0;36mpandas._libs.lib.map_infer\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m<ipython-input-2-4a4188a25a16>\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(row)\u001b[0m\n\u001b[1;32m     88\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mquery\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtest_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitertuples\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m     similarities = train_df['content'].swifter.progress_bar(False).apply(\n\u001b[0;32m---> 90\u001b[0;31m         lambda row: jaccard_similarity(row, query.content))\n\u001b[0m\u001b[1;32m     91\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0many\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msimilarities\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0mTHRESHOLD\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m         \u001b[0mjaccard_duplicates\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-2-4a4188a25a16>\u001b[0m in \u001b[0;36mjaccard_similarity\u001b[0;34m(query, document)\u001b[0m\n\u001b[1;32m     78\u001b[0m     \u001b[0mquery_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mquery\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     79\u001b[0m     \u001b[0mdocument_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdocument\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 80\u001b[0;31m     \u001b[0mintersection\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mquery_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mintersection\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdocument_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     81\u001b[0m     \u001b[0munion\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mquery_list\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mdocument_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     82\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mintersection\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0munion\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import time\n",
    "import swifter\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from collections import defaultdict\n",
    "from itertools import combinations\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from nltk import jaccard_distance\n",
    "from datasketch import MinHashLSH, MinHash\n",
    "\n",
    "THRESHOLD = 0.8\n",
    "\n",
    "# Read data\n",
    "train_df = pd.read_csv('../assist_material/datasets/extracted/datasets2020/datasets/q2a/corpusTrain.csv', sep=',')\n",
    "train_df.columns = ['id', 'content']\n",
    "\n",
    "test_df = pd.read_csv('../assist_material/datasets/extracted/datasets2020/datasets/q2a/corpusTest.csv', sep=',')\n",
    "test_df.columns = ['id', 'content']\n",
    "\n",
    "# Preprocess\n",
    "def preprocess(df):\n",
    "    \"\"\"\n",
    "    Cleans up a given dataframe\n",
    "    :param df: The dataframe to clean\n",
    "    :return: The cleaned dataframe\n",
    "    \"\"\"\n",
    "\n",
    "    _df = df.copy(deep=True)\n",
    "    _df['content'] = _df['content'].str.lower()\n",
    "    _df['content'] = _df['content'].swifter.apply(lambda row: row.encode('ascii',errors='ignore').decode())\n",
    "    _df = _df[_df['content'].str.split().str.len().gt(2)]\n",
    "    _df['content'] = _df['content'].str.replace(r'[\\n\\'\\\"?\\.,:\\(\\)]', '', regex=True)\n",
    "    _df.reset_index(drop=True, inplace=True)\n",
    "    return _df\n",
    "\n",
    "train_df = preprocess(train_df)\n",
    "test_df = preprocess(test_df)\n",
    "\n",
    "# Vectorize texts\n",
    "vectorizer = TfidfVectorizer()\n",
    "\n",
    "train_tfidf = vectorizer.fit_transform(train_df['content'])\n",
    "\n",
    "\n",
    "# Exact Cosine\n",
    "# cosine_query_start_time = time.time()\n",
    "# cosine_duplicates = 0\n",
    "\n",
    "\n",
    "# for query in test_df.itertuples(index=False):\n",
    "#     query_tfidf = vectorizer.transform([query.content])\n",
    "\n",
    "#     similarities = cosine_similarity(query_tfidf, train_tfidf).flatten()\n",
    "#     if np.any(similarities > THRESHOLD):\n",
    "#         cosine_duplicates += 1\n",
    "\n",
    "#     indexes = np.where(similarities > THRESHOLD)\n",
    "#     print(f\"Query: {query}\")\n",
    "#     for index in indexes:\n",
    "#         print(train_df['content'][index])\n",
    "\n",
    "# cosine_query_end_time = time.time()\n",
    "# print(f'Total duplicates found: {cosine_duplicates}')\n",
    "# print(f'Cosine Similarity query time {(cosine_query_end_time - cosine_query_start_time):.2f} seconds')\n",
    "\n",
    "\n",
    "# Exact Jaccard\n",
    "def jaccard_similarity(query, document):\n",
    "    \"\"\"\n",
    "    This method estimates the Jaccard similarity between two documents\n",
    "\n",
    "    :param query: The first document\n",
    "    :param document: The second document\n",
    "    :return: The similarity between the two documents\n",
    "    \"\"\"\n",
    "    query_list = query.split()\n",
    "    document_list = document.split()\n",
    "    s1, s2 = set(list1), set(list2)\n",
    "    return len(s1 & s2) / len(s1 | s2)\n",
    "\n",
    "jaccard_query_start_time = time.time()\n",
    "jaccard_duplicates = 0\n",
    "\n",
    "for query in test_df.itertuples(index=False):\n",
    "    similarities = train_df['content'].swifter.progress_bar(False).apply(\n",
    "        lambda row: jaccard_similarity(row, query.content))\n",
    "    if np.any(similarities > THRESHOLD):\n",
    "        jaccard_duplicates += 1\n",
    "\n",
    "    indexes = np.where(similarities > THRESHOLD)\n",
    "    print(f'Query: {query}')\n",
    "    for index in indexes:\n",
    "        print(train_df['content'][index])\n",
    "\n",
    "\n",
    "jaccard_query_end_time = time.time()\n",
    "print(f'Total duplicates found: {jaccard_duplicates}')\n",
    "print(f'Jaccard Similarity query time {(jaccard_query_end_time - jaccard_query_start_time):.2f} seconds')\n",
    "\n",
    "\n",
    "# Random Projection LSH with Cosine Similarity\n",
    "\n",
    "def random_vectors_generator(dimension, n_vectors):\n",
    "    \"\"\"\n",
    "    Generates a collection of random vectors from the standard Gaussian distribution.\n",
    "    :param dimension: The dimension of the vector\n",
    "    :param n_vectors: The number of vectors\n",
    "    :return: Array with random vectors\n",
    "    \"\"\"\n",
    "    return np.random.randn(dimension, n_vectors)\n",
    "\n",
    "\n",
    "def train_lsh(X_tfidf, n_vectors):\n",
    "    \"\"\"\n",
    "    Method that creates a LSH model given the TFIDF vector to train\n",
    "    :param X_tfidf: The TFIDF vector to train the model\n",
    "    :param n_vectors: The number of random vectors to generate\n",
    "    :return: The model\n",
    "    \"\"\"\n",
    "    np.random.seed(0)\n",
    "    dim = X_tfidf.shape[1]\n",
    "    random_vectors = random_vectors_generator(dim, n_vectors)\n",
    "\n",
    "    # Partition data points into bins and encode bin index bits into integers\n",
    "    bin_indices_bits = X_tfidf.dot(random_vectors) >= 0\n",
    "    # Compute the dot product between the document vector and the vector consisting of powers of 2\n",
    "    # x << y is the same as multiplying x by 2 ** y\n",
    "    powers_of_two = 1 << np.arange(n_vectors - 1, -1, step=-1)\n",
    "    # Final integer representation of individual bins\n",
    "    bin_indices = bin_indices_bits.dot(powers_of_two)\n",
    "\n",
    "    # Update `table` so that `table[i]` is the list of document ids with bin index equal to i\n",
    "    table = defaultdict(list)\n",
    "    for idx, bin_index in enumerate(bin_indices):\n",
    "        table[bin_index].append(idx)\n",
    "\n",
    "    model = {'table': table,\n",
    "             'random_vectors': random_vectors,\n",
    "             'bin_indices': bin_indices,\n",
    "             'bin_indices_bits': bin_indices_bits}\n",
    "    return model\n",
    "\n",
    "\n",
    "def search_nearby_bins(query_bin_bits, table, search_radius, candidate_set):\n",
    "    \"\"\"\n",
    "    For a given query vector and trained LSH model's table\n",
    "    return all candidate neighbors with the specified search radius.\n",
    "\n",
    "    Example\n",
    "    -------\n",
    "    model = train_lsh(X_tfidf, n_vectors=16)\n",
    "    query = model['bin_index_bits'][0]  # vector for the first document\n",
    "    candidates = search_nearby_bins(query, model['table'])\n",
    "    :param query_bin_bits: The binary representation of the query document\n",
    "    :param table: The trained hash table\n",
    "    :param search_radius: The nearby bins to search\n",
    "    :param candidate_set: The set that holds the candidate neighbours\n",
    "    :return: The set that holds the candidate neighbours\n",
    "    \"\"\"\n",
    "    if candidate_set is None:\n",
    "        candidate_set = set()\n",
    "\n",
    "    n_vectors = query_bin_bits.shape[0]\n",
    "    powers_of_two = 1 << np.arange(n_vectors - 1, -1, step=-1)\n",
    "\n",
    "    for different_bits in combinations(range(n_vectors), search_radius):\n",
    "        # Flip the bits (n_1, n_2, ..., n_r) of the query bin to produce a new bit vector\n",
    "        index = list(different_bits)\n",
    "        alternate_bits = query_bin_bits.copy()\n",
    "        alternate_bits[index] = np.logical_not(alternate_bits[index])\n",
    "\n",
    "        # Convert the new bit vector to an integer index\n",
    "        nearby_bin = alternate_bits.dot(powers_of_two)\n",
    "\n",
    "        # fetch the list of documents belonging to\n",
    "        # the bin indexed by the new bit vector,\n",
    "        # then add those documents to candidate_set;\n",
    "        # make sure that the bin exists in the table\n",
    "        if nearby_bin in table:\n",
    "            candidate_set.update(table[nearby_bin])\n",
    "\n",
    "    return candidate_set\n",
    "\n",
    "\n",
    "def get_nearest_neighbors(X_tfidf, query_vector, model, max_search_radius=3):\n",
    "    \"\"\"\n",
    "    Business method that returns the approximate nearest neighbors of a given document. Here it is calculated the bit\n",
    "    index of the document to search and using the methods above we extract the neighbors along with the cosine\n",
    "    similarity with descending order.\n",
    "    :param X_tfidf: The train TFIDF\n",
    "    :param query_vector: The document to search TFIDF\n",
    "    :param model: The LSH model\n",
    "    :param max_search_radius: The nearby bins to search\n",
    "    :return: List with nearest neighbours with cosine similarities\n",
    "    \"\"\"\n",
    "    table = model['table']\n",
    "    random_vectors = model['random_vectors']\n",
    "\n",
    "    # Compute bin index for the query vector, in bit representation.\n",
    "    bin_index_bits = np.ravel(query_vector.dot(random_vectors) >= 0)\n",
    "\n",
    "    # Search nearby bins and collect candidates\n",
    "    candidate_set = set()\n",
    "    for search_radius in range(max_search_radius + 1):\n",
    "        candidate_set = search_nearby_bins(bin_index_bits, table, search_radius, candidate_set)\n",
    "\n",
    "    # Sort candidates by their true distances from the query\n",
    "    candidate_list = list(candidate_set)\n",
    "    candidates = X_tfidf[candidate_list]\n",
    "    similarities = cosine_similarity(candidates, query_vector).flatten()\n",
    "\n",
    "    similarities_col = 'similarities'\n",
    "    nearest_neighbors = pd.DataFrame({\n",
    "        'id': candidate_list, similarities_col: similarities\n",
    "    }).sort_values(similarities_col, ascending=False).reset_index(drop=True)\n",
    "    return nearest_neighbors\n",
    "\n",
    "\n",
    "# lsh_cosine_build_start_time = time.time()\n",
    "# lsh_model = train_lsh(train_tfidf, 16)\n",
    "# lsh_cosine_build_end_time = time.time()\n",
    "#\n",
    "# lsh_query_times = []\n",
    "# lsh_duplicates = []\n",
    "# for k in range (1, 10 + 1):\n",
    "#     lsh_cosine_query_start_time = time.time()\n",
    "#     duplicates = 0\n",
    "#     for query in test_df.itertuples(index=False):\n",
    "#         query_tfidf = vectorizer.transform([query.content])\n",
    "#         nearest_neighbors = get_nearest_neighbors(train_tfidf, query_tfidf, lsh_model, max_search_radius=k)\n",
    "#         # If the max value is above 0.8 consider it duplicate\n",
    "#         if nearest_neighbors['similarities'][0] > THRESHOLD:\n",
    "#             duplicates += 1\n",
    "#\n",
    "#         indexes = nearest_neighbors[nearest_neighbors['similarities'] > THRESHOLD]\n",
    "#         print(f'Query: {query}')\n",
    "#         for index in indexes.itertuples(index=False):\n",
    "#             print(train_df['content'][index.id])\n",
    "#\n",
    "#     lsh_cosine_query_end_time = time.time()\n",
    "#     lsh_query_times.append(lsh_cosine_query_end_time - lsh_cosine_query_start_time)\n",
    "#     print(f'Total duplicates found: {duplicates} for k = {k}')\n",
    "#     print(f'LSH projection Cosine Similarity query time {(lsh_cosine_query_end_time - lsh_cosine_query_start_time):.2f} '\n",
    "#           f'seconds')\n",
    "\n",
    "# Min-Hash LSH Jaccard with Similarity\n",
    "\n",
    "def train_min_lsh(X, n_permutations):\n",
    "    \"\"\"\n",
    "    Method that creates a Min-LSH model given the training dataframe\n",
    "    :param X: The train dataframe\n",
    "    :param n_permutations: The number of permutations to apply\n",
    "    :return: The model\n",
    "    \"\"\"\n",
    "    model = MinHashLSH(threshold=THRESHOLD, num_perm=n_permutations)\n",
    "    for entry in X.itertuples(index=False):\n",
    "        min_hash = MinHash(num_perm=n_permutations)\n",
    "        set_text = set(entry.content.split())\n",
    "        for d in set_text:\n",
    "            min_hash.update(d.encode('utf8'))\n",
    "\n",
    "        model.insert(entry.id, min_hash)\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "# lsh_model = train_min_lsh(train_df, 16)\n",
    "\n",
    "# for query in test_df.itertuples(index=False):\n",
    "#     min_hash = MinHash(num_perm=16)\n",
    "#     set_text = set(query.content.split())\n",
    "#     for d in set_text:\n",
    "#         min_hash.update(d.encode('utf8'))\n",
    "#     result = lsh_model.query(min_hash)\n",
    "#     print('Query', query)\n",
    "#     for index in result:\n",
    "#         print(train_df.loc[train_df['id'] == index])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
